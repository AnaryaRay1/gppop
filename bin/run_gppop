#!/usr/bin/env python
import argparse
from gppop import core, io
import numpy as np
import pymc as pm
import os
import sys
import yaml
import h5py
try:
    from pymc import sampling_jax
except ModuleNotFoundError as e:
    print(e)
    print("JAX not installed, falling back to normal sampling")
    nojax = True

np.random.seed(123456)
parser = argparse.ArgumentParser(description = 'Fit the Gaussian Process model to the observed source population.', formatter_class = argparse.ArgumentDefaultsHelpFormatter, fromfile_prefix_chars='@')
parser.add_argument('--config', type = str, required = True, metavar = 'CONFIG', help = 'path to config file')
parser.add_argument('--ntune', type = int, required = True, metavar = 'NTUNE', help = 'Number of tuning steps for the sampler.')
parser.add_argument('--nsteps', type = int, required = True, metavar = 'NSTEPS', help = 'Number of steps to sample post tuning.')
parser.add_argument('--target_accept', type = float, required = True, metavar = 'TARGET_ACCEPT', help = 'Target acceptance fraction for the sampler.')
parser.add_argument('--njobs', type = int, required = True, metavar = 'TARGET_ACCEPT', help = 'number of chains to run sampling for')
parser.add_argument('--jax_sampling', required = False, metavar = 'JAX_SAMPLING', help = 'whether to use jax sampling or not',type=bool,default=False)
parser.add_argument('--sigma_sd', required = False, metavar = 'SIGMA_SD', help = 'the sd of the prior on GP sigma',type=float,default=10)
parser.add_argument('--mu_dim', required = False, metavar = 'MU_DIM', help = 'the dimensionality of the mean of the redshift GP for uncorrelated inference or of the full GP in correlated inference',type=int,default=None)
parser.add_argument('--vt_accuracy_check', required = False, metavar = 'VT_ACCURACY_CHECK', help = 'Whether to marginalize over monte-carlo uncertainties in vt estimation',type=bool,default=False)
parser.add_argument('--meta_file', required = False, metavar = 'path to popsummary metafile.', help = 'If provided, only output will be overwritten',type=bool,default=None)
args= parser.parse_args()

with open(args.config, "r") as stream:
    config = yaml.full_load(stream)

if args.meta_file is None:
    io.create_metafile(np.array(config['mbins']),np.array(config['zbins']),config['meta_file'],config['nsamples'], 
                         config['injection_file'], config['injection_keys'], config['threshold'], config['threshold_keys'],
                         event_dict = config['event_dict'],
                         pe_summary_event_dict = config['pe_summary_event_dict'],
                         analysis_type = config['analysis_type'])
else:
    pass

mbins, zbins = np.array(config['mbins']),np.array(config['zbins'])

utils = core.Utils(mbins,zbins)
rates = core.Rates(mbins,zbins)

log_bin_centers = utils.generate_log_bin_centers()
deltaLogbin = utils.deltaLogbin()
tril_deltaLogbin = utils.arraynd_to_tril(deltaLogbin)

with h5py.File(config['meta_file'],'r') as hf:
    gppop_data = hf['gppop_metadata']
    weights, total_vts, total_vt_sigmas = gppop_data['posterior_weights'][()], gppop_data['vt_means'][()], gppop_data['vt_sigmas'][()]


nbins = len(log_bin_centers)

nz= len(zbins)-1
nm = int(len(log_bin_centers)/nz)

dist_array = np.zeros(int(nm*(nm+1)/2))

z_bin_centers = log_bin_centers[0::nm,2]
logm_bin_centers = log_bin_centers[:nm,:2]
k=0
for i in range(len(logm_bin_centers)):
    for j in range(i+1):
        dist_array[k] = np.linalg.norm(logm_bin_centers[i]-logm_bin_centers[j])
        k+=1

scale_min = np.log(np.min(dist_array[dist_array!=0.]))
scale_max = np.log(np.max(dist_array))
scale_mean_m = 0.5*(scale_min + scale_max) # chosen to give coverage over the bin-grid
scale_sd_m = (scale_max - scale_min)/4 # fix 3-sigma difference to the sd of the length scale dist

dist_array = np.zeros(int(nz*(nz+1)/2))
k=0
for i in range(len(z_bin_centers)):
    for j in range(i+1):
        dist_array[k] = np.linalg.norm(z_bin_centers[i]-z_bin_centers[j])
        k+=1

scale_min = np.log(np.min(dist_array[dist_array!=0.]))
scale_max = np.log(np.max(dist_array))
scale_mean_z = 0.5*(scale_min + scale_max) # chosen to give coverage over the bin-grid
scale_sd_z = (scale_max - scale_min)/4 # fix 3-sigma difference to the sd of the length scale dist
corr_posterior_model = rates.make_significant_model_3d
corr_prior_model = rates.make_gp_prior_model_3d





if config['analysis_type'] != 'uncor':
    gp_model = corr_posterior_model(log_bin_centers=log_bin_centers,weights=weights,tril_vts=total_vts,tril_deltaLogbins=tril_deltaLogbin,ls_mean_m=scale_mean_m,ls_sd_m=scale_sd_m,ls_mean_z=scale_mean_z,ls_sd_z=scale_sd_z, sigma_sd=args.sigma_sd,mu_dim=args.mu_dim,vt_sigmas = total_vt_sigmas, vt_accuracy_check = args.vt_accuracy_check)
    gp_model_prior = corr_prior_model(log_bin_centers = log_bin_centers, ls_mean_m=scale_mean_m,ls_sd_m=scale_sd_m,ls_mean_z=scale_mean_z,ls_sd_z=scale_sd_z, sigma_sd=args.sigma_sd,mu_dim=args.mu_dim)

else:
    gp_model = rates.make_significant_model_3d_evolution_only(log_bin_centers=log_bin_centers,weights=weights,tril_vts=total_vts,tril_deltaLogbins=tril_deltaLogbin,ls_mean_m=scale_mean_m,ls_sd_m=scale_sd_m,ls_mean_z=scale_mean_z,ls_sd_z=scale_sd_z, sigma_sd=args.sigma_sd,mu_z_dim=args.mu_dim,vt_sigmas = total_vt_sigmas, vt_accuracy_check = args.vt_accuracy_check )
    gp_model_prior = rates.make_gp_prior_model_3d_evolution_only(log_bin_centers = log_bin_centers, ls_mean_m=scale_mean_m,ls_sd_m=scale_sd_m,ls_mean_z=scale_mean_z,ls_sd_z=scale_sd_z, sigma_sd=args.sigma_sd,mu_z_dim=args.mu_dim)


print(scale_mean_m,scale_sd_m,scale_mean_z,scale_sd_z,args.jax_sampling,args.sigma_sd)

print('start sampling prior')
with gp_model_prior:
    if args.jax_sampling and not nojax:
        trace = pm.sampling_jax.sample_numpyro_nuts(draws=args.nsteps,tune=args.ntune,chains=1 ,target_accept=args.target_accept)
    else:
        trace = pm.sample(draws=args.nsteps,tune=args.ntune,chains=args.njobs,target_accept=args.target_accept,discard_tuned_samples=True)

trace.to_netcdf('gppop_prior.nc')

print('start sampling posterior')
with gp_model:
    if args.jax_sampling and not nojax:
        trace = pm.sampling_jax.sample_numpyro_nuts(draws=args.nsteps,tune=args.ntune,chains=1 ,target_accept=args.target_accept)
    else:
        trace = pm.sample(draws=args.nsteps,tune=args.ntune,chains=args.njobs,target_accept=args.target_accept,discard_tuned_samples=True)


trace.to_netcdf('gppop_posterior.nc')

io.write_results_to_metafile(config['meta_file'] if args.meta_file is None else args.meta_file, 'gppop_posterior.nc','gppop_prior.nc', config['n_draw_pe'], config['n_draw_inj'], config['n_draw_pred'], overwrite = True, include_spins = True)

os.remove('gppop_posterior.nc')
os.remove('gppop_prior.nc')